from cot_prompts import construct_cot_prompt_srl, construct_cot_prompt_coref
from evaluate import evaluate
from graph import construct_graph, Graph
from utils import Config, analyse_beams, restrict_vocab
from plot import plot_yes_no, plot_calibration, plot_score_diff
from preprocess import preprocess_file
from prompts import generate_prompts
from inference import run_inference

import argparse
import numpy as np
import pandas as pd
import pickle
from pathlib import Path
import openai
import torch
from tqdm import tqdm
from transformers import T5Tokenizer, T5ForConditionalGeneration
from typing import Union
import time

GPU_ID = "0"
device = torch.device(f"cuda:{GPU_ID}" if torch.cuda.is_available() else "cpu")
torch.manual_seed(2121)



class PromptModel():
    def __init__(self, config: Config):
        """ Constructor for the Prompt Model Pipeline. Preprocesses the data 
        to break it down into components of a structure. Each row essentially
        should have the essential ingredients for constructing the prompt.
        Also, intiializes the prompt model.
        Inputs
        -----------------------
        config- utils.Config. A config dictionary which loads the meta-data and 
                paramaeters from the confg.ini file. 
        """
        self.config = config
        self.data = preprocess_file(
                    file_path = Path(self.config.data_dir, self.config.data_file),
                    task = self.config.task_name,
                    dataset = self.config.dataset_name
                )
        
        print(f"Total number of queries: {len(self.data)}")



    def generate_prompts_gpt(self):
        """ Generating the prompts for GPT-4.
        Outputs
        -----------------
        prompts: List[str]. A list of prompts to the model
        gold: List[str]. A list of gold answers to each of the prompts. Parallel list to'prompts'

        """
        prompts = []    # Will contain all the prompts
        gold = []       # Will contain all the gold answers
        predicate = None

        if self.config.task_name in ["srl"]:
            for ix, row in self.data.iterrows():
                # Intiialize running attributes 
                if predicate == None:
                    predicate = row['predicate']
                    sent_id = row["sent_id"]
                    q_ix = 1
                # Indicates completion of a structure. Resetting question ID
                if (predicate != row["predicate"]) or (sent_id != row["sent_id"]):
                    predicate = row['predicate']
                    sent_id = row["sent_id"]
                    q_ix = 1
                # Append the prompt and its gold answer and update the question ID
                prompts.append(f"""Question {q_ix}: {row["question"]}""")
                gold.append(row["answer"]) 
                q_ix += 1
        else:
            raise ValueError("GPT-4 experiments are only supported for SRL. Please check the config files.")
                
        return prompts, gold



    def generate(self):
        """ Method to prepare prompts and generate.
        Outputs
        ------------
        prompts: List[str]. A list of prompts to the model
        gold: List[str]. A list of gold answers to each of the prompts. Parallel list to'prompts'
        generation: List[str]. A list of answers generated by the model. Parallel list to 'prompts'
        """
        prompts, gold = self.generate_prompts_gpt()    # Generate prompts and their gold answers    
        generation = [] # The list contains all the generation from the model
        sleep_ix = 0

        inst = "You will be given a set of questions regarding a particular sentence. Answer these questions such that the answers to the questions do not overlap. Answers should strictly be a subsequence of the sentence. Do not include anything except the answer phrase in the answer." 
    
        # Iterate over prompts
        predicate = None
        prompts_so_far = []
        ### Uncomment the following if the API throws an error a request limit error
        ### The sleep_ix should be set as the index of the row at which the the error was encountered in the 
        ### plus 40.
        ### previous run. The generation loop creates a dump at roughly every 40 questions. 
        ### Also uncomment the if condition under the next for loop.
        #sleep_ix = 44052
        #with open("./../dumps/gpt_gens_44022.bin","rb") as inp_f:
        #    generation = pickle.load(inp_f)
        #    print(len(generation))

        for ix, row in tqdm(self.data.iterrows()):
            ### Uncomment if request limit error encountered
            ### Set the value as the index of the row at which the error occurred 
            #if ix < 44022:
            #    continue
            if self.config.task_name == "srl":
                # Intialize all the running attributes during the first iteration
                if predicate == None:
                    predicate = row['predicate']    # The SRL predicate
                    sent_id = row["sent_id"]        # SRL Sentence ID 
                    sentence = row["sentence"]      # The SRL sentence
                    # Convert the sentence to string if split by tokens
                    # and vice-versa
                    if type(sentence) == list:
                        sent_toks = row["sentence"] # Contains space separated token list
                        sentence = " ".join(sentence)
                    else:
                        sent_toks = row["sentence"].split()
                    prompts_so_far = []

                # This condition becomes true when a predicate changes indicating the compeletion
                # of a structure
                if (predicate != row["predicate"]) or (sent_id != row["sent_id"]):                    
                    # Construct the prompt 
                    # Please read the appendix of the paper to see the prompt template
                    user_prompt = f"""Sentence: {sentence}\n\n"""
                    for pr in prompts_so_far:
                        user_prompt = user_prompt + pr + "\n"
                    message_history = [{"role": "system", "content": inst},{"role": "user", "content": user_prompt}]
                    # Querying GPT-4
                    response = openai.ChatCompletion.create(model="gpt-4", messages=message_history, max_tokens=500)
                    gpt_ans = response["choices"][0]["message"]["content"]  # Extarct the best response
                    print(user_prompt)
                    print(gpt_ans)

                    # Intialize the prompt with a null response. The length of the list is equal to the 
                    # number of questions a.k.a roles
                    ans = ["Has a none response ."]*len(prompts_so_far)
                    # Post-processing answers. Disentangling the answers since it is a single string. 
                    for gpt_a in gpt_ans.split("\n"):
                        # The logic below works if more than one question is asked
                        if len(gpt_a.split(':')) > 1:
                            try:
                                num = int(gpt_a.split(':')[0].split()[-1])-1  #Answer to the question
                            except ValueError:
                                ans[0] = gpt_a.strip()
                                continue
                            # This is the most frequent case
                            if len(gpt_a.split(':')) == 2:
                                ans[num] = gpt_a.split(':')[1].strip()
                            else:
                                ans[num] = " ".join([a.strip() for a in gpt_a.split(':')[1:]])
                    # If there is only one question, post-processing is done in this fashion
                    if (len(ans) == 1):
                        if len(gpt_ans.split(':')) > 1:
                            ans[0] = gpt_ans.split(':')[1].strip()
                        else:
                            ans[0] = gpt_ans.strip()
                    print(ans)
                    print("\n\n")
                    
                    generation.extend(ans.copy())
                    
                    # Delay for a minute to avoid request limit errors and create dumps 
                    # for the processed structures
                    if ix > sleep_ix:
                        time.sleep(60)
                        sleep_ix += 40
                        with open(f"./../dumps/gpt_gens_{ix}.bin","wb") as out:
                            pickle.dump(generation, out)
             
                    # Change all the running parameters 
                    predicate = row['predicate']
                    sent_id = row["sent_id"]
                    sentence = row["sentence"]
                    
                    if type(sentence) == list:
                        sent_toks = row["sentence"]
                        sentence = " ".join(sentence)
                    else:
                        sent_toks = row["sentence"].split()
                    prompts_so_far = []
            
            prompts_so_far.append(prompts[ix])
        
        ### Similar to above, process the last structure
        user_prompt = f"""Sentence: {sentence}\n\n"""
        for pr in prompts_so_far:
            user_prompt = user_prompt + pr + "\n"
        message_history = [{"role": "system", "content": inst},{"role": "user", "content": user_prompt}]
        response = openai.ChatCompletion.create(model="gpt-4", messages=message_history, max_tokens=500)
        gpt_ans = response["choices"][0]["message"]["content"]
        print(user_prompt)
        print(gpt_ans)
        ans = ["Has a none response ."]*len(prompts_so_far)
        for gpt_a in gpt_ans.split("\n"):
            if len(gpt_a.split(':')) > 1:
                num = int(gpt_a.split(':')[0].split()[-1])-1  #Answer to the question
                if len(gpt_a.split(':')) == 2:
                    ans[num] = gpt_a.split(':')[1].strip()
                else:
                    ans[num] = " ".join([a.strip() for a in gpt_a.split(':')[1:]])
                    
        if (len(ans) == 1):
            if len(gpt_ans.split(':')) > 1:
                ans[0] = gpt_ans.split(':')[1].strip()
            else:
                ans[0] = gpt_ans.strip()
        print(ans)
        print("\n\n")
                    
        generation.extend(ans.copy())
                    
        return prompts, gold, generation




def add_parser_args(parser):
    parser.add_argument('--config_file', default= "config.ini", type=str)
    parser.add_argument('--read_generated', action='store_true')
    parser.add_argument('--read_inferences', action='store_true')
    return parser


if __name__ == "__main__":
    
    parser = argparse.ArgumentParser()
    parser = add_parser_args(parser)
    args = vars(parser.parse_args())
    
    config = Config(filename=args['config_file'])
    model = PromptModel(config)
    
    # Intermediary dump data
    task_name = config.task_name           #"srl"
    dataset_name = config.dataset_name     #e.g.:"qasrl2"
    model_name = config.model              #e.g.:"flan-t5-xl"
    read_spec = config.read_spec            #e.g.:"highlight_fullcontext"
    spec_det = config.spec_det              #e.g.:"highlight_fullcontext_rtol"
    read_file_infix = f"{model_name}{read_spec}"
    file_infix = f"{model_name}{spec_det}"
    
    run_generate = not args['read_generated']
    run_inference_module = not args['read_inferences']

    ####### STEP 1. Generation
    ### Generate & dump generations and gold
    if run_generate:
        _, gold, gens = model.generate() 
        with open(f"./../dumps/{dataset_name}_{task_name}_{file_infix}_gens.bin","wb") as out:
            pickle.dump(gens, out)
        with open(f"./../dumps/{dataset_name}_{task_name}_{file_infix}_gold","wb") as out:
            pickle.dump(gold, out) 
    ### Read Dumps
    else:
        with open(f"./../dumps/{dataset_name}_{task_name}_{read_file_infix}_gens.bin","rb") as out:
            gens = pickle.load(out)
        with open(f"./../dumps/{dataset_name}_{task_name}_{read_file_infix}_gold","rb") as out:
            gold = pickle.load(out)    


    ## Unconstrained Evaluation
    print("GPT-4 with Verbal constraints")
    meta = {"gold_dump_file": f"./../results/coref/{dataset_name}_{file_infix}_gold.txt", "pred_dump_file": f"./../results/coref/{dataset_name}_{file_infix}_uncons.txt", "constrained":False}
    evaluate(model.data, model.config, gens, meta)
    

